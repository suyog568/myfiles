{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1sO2foOOKvRhSn1Pe5WH7nYx5R2fu1O83",
      "authorship_tag": "ABX9TyO1HR797cOxkcnwDpQvqnLP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suyog568/myfiles/blob/main/Worksheet_Week5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Name : Suyog Bastakoti\n",
        "\n",
        "University ID: 2407093\n",
        "\n",
        "Module: Concepts and Technologies of AI\n"
      ],
      "metadata": {
        "id": "I-cwIUAxzbQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtHyJs91zxfg",
        "outputId": "3447ff3b-fc50-4534-c00e-5270b44e442e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Read and Observe the Dataset\n",
        "First, we will import the necessary libraries and read the CSV file.\n"
      ],
      "metadata": {
        "id": "RK9qphTGYVGw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/drive/MyDrive/Data Set /student.csv')\n"
      ],
      "metadata": {
        "id": "ShUugFisZSCq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Print Top (5) and Bottom (5) of the Dataset\n",
        "Next, we can use pd.head() and pd.tail() to display the first and last five entries of the dataset."
      ],
      "metadata": {
        "id": "qoaLxm7zZYLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print top 5 entries\n",
        "print(\"Top 5 entries:\")\n",
        "print(data.head())\n",
        "\n",
        "# Print bottom 5 entries\n",
        "print(\"\\nBottom 5 entries:\")\n",
        "print(data.tail())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGSGZbO3ZaD-",
        "outputId": "eb7288bb-d3f6-401a-9c9a-1f10f82222a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 entries:\n",
            "   Math  Reading  Writing\n",
            "0    48       68       63\n",
            "1    62       81       72\n",
            "2    79       80       78\n",
            "3    76       83       79\n",
            "4    59       64       62\n",
            "\n",
            "Bottom 5 entries:\n",
            "     Math  Reading  Writing\n",
            "995    72       74       70\n",
            "996    73       86       90\n",
            "997    89       87       94\n",
            "998    83       82       78\n",
            "999    66       66       72\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Print the Information of Datasets\n",
        "We will use pd.info() to get a concise summary of the DataFrame, which includes the number of non-null entries and data types."
      ],
      "metadata": {
        "id": "VQxhvfdTZcs5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print information about the dataset\n",
        "print(\"\\nDataset Information:\")\n",
        "print(data.info())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVYerNZJZflu",
        "outputId": "b644d00c-e25f-4134-a442-6fe4963dcd68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset Information:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 3 columns):\n",
            " #   Column   Non-Null Count  Dtype\n",
            "---  ------   --------------  -----\n",
            " 0   Math     1000 non-null   int64\n",
            " 1   Reading  1000 non-null   int64\n",
            " 2   Writing  1000 non-null   int64\n",
            "dtypes: int64(3)\n",
            "memory usage: 23.6 KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Gather Descriptive Info about the Dataset\n",
        "To gather descriptive statistics, we will use pd.describe(), which provides count, mean, standard deviation, min, max, and quartiles."
      ],
      "metadata": {
        "id": "dmxFGYOCZilG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get descriptive statistics\n",
        "print(\"\\nDescriptive Statistics:\")\n",
        "print(data.describe())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-rEGZDPZkaD",
        "outputId": "5f0fe3cd-8573-4919-ff34-966ec978b1e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Descriptive Statistics:\n",
            "              Math      Reading      Writing\n",
            "count  1000.000000  1000.000000  1000.000000\n",
            "mean     67.290000    69.872000    68.616000\n",
            "std      15.085008    14.657027    15.241287\n",
            "min      13.000000    19.000000    14.000000\n",
            "25%      58.000000    60.750000    58.000000\n",
            "50%      68.000000    70.000000    69.500000\n",
            "75%      78.000000    81.000000    79.000000\n",
            "max     100.000000   100.000000   100.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Split Your Data into Feature (X) and Label (Y)\n",
        "Finally, we will separate our features (Math and Reading scores) from our label (Writing scores)."
      ],
      "metadata": {
        "id": "sJyz7sUmZnSa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into features (X) and label (Y)\n",
        "X = data[['Math', 'Reading']]\n",
        "Y = data['Writing']\n"
      ],
      "metadata": {
        "id": "EbJU67ueZpak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To - Do - 2:\n",
        "1. To make the task easier - let’s assume there is no bias or intercept.\n",
        "2. Create the following matrices:\n",
        "\n",
        "Y = WTX\n",
        "\n",
        "W =\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "w1\n",
        "w2\n",
        ".\n",
        ".\n",
        ".\n",
        "wd\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ", where W ∈ R\n",
        "d\n",
        "\n",
        "X =\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "x1,1 x1,2 · · · x1,n\n",
        "x2,1 x2,2 · · · x2,n\n",
        ".\n",
        ".\n",
        ".\n",
        ".\n",
        ".\n",
        ".\n",
        ".\n",
        ".\n",
        ".\n",
        ".\n",
        ".\n",
        ".\n",
        "xd,1 xd,2 · · · xd,n\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ", where X ∈ R\n",
        "d×n\n",
        "\n",
        "Y =\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "y1\n",
        "y2\n",
        ".\n",
        ".\n",
        ".\n",
        "yn\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ", where Y ∈ R\n",
        "n\n",
        "\n",
        "3. Note: The feature matrix described above does not include a column of 1s, as it assumes the\n",
        "absence of a bias term in the model."
      ],
      "metadata": {
        "id": "LVg0vujiaDl5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Create Matrix Y\n",
        "Y = data['Writing'].values.reshape(-1, 1)  # Reshape to a column vector\n",
        "\n",
        "# Step 2: Define Matrix W (weights)\n",
        "# Assuming weights are initialized to some values; here we use random values for demonstration\n",
        "W = np.random.rand(2, 1)  # Two weights for Math and Reading\n",
        "\n",
        "# Step 3: Create Matrix X (features)\n",
        "X = data[['Math', 'Reading']].values.T  # Transpose to get shape (d, n)\n",
        "\n",
        "# Display the matrices\n",
        "print(\"Matrix Y (Writing Scores):\")\n",
        "print(Y)\n",
        "\n",
        "print(\"\\nMatrix W (Weights):\")\n",
        "print(W)\n",
        "\n",
        "print(\"\\nMatrix X (Features):\")\n",
        "print(X)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MbU4AiBcLwt",
        "outputId": "e978e07c-fd42-4db7-fb27-330cfbc54f83"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix Y (Writing Scores):\n",
            "[[ 63]\n",
            " [ 72]\n",
            " [ 78]\n",
            " [ 79]\n",
            " [ 62]\n",
            " [ 85]\n",
            " [ 83]\n",
            " [ 41]\n",
            " [ 80]\n",
            " [ 77]\n",
            " [ 64]\n",
            " [ 90]\n",
            " [ 45]\n",
            " [ 77]\n",
            " [ 70]\n",
            " [ 46]\n",
            " [ 76]\n",
            " [ 44]\n",
            " [ 85]\n",
            " [ 72]\n",
            " [ 53]\n",
            " [ 66]\n",
            " [ 75]\n",
            " [ 49]\n",
            " [ 84]\n",
            " [ 83]\n",
            " [ 68]\n",
            " [ 66]\n",
            " [ 77]\n",
            " [ 78]\n",
            " [ 74]\n",
            " [ 83]\n",
            " [ 72]\n",
            " [ 65]\n",
            " [ 46]\n",
            " [ 66]\n",
            " [ 50]\n",
            " [ 79]\n",
            " [ 68]\n",
            " [ 46]\n",
            " [ 86]\n",
            " [ 70]\n",
            " [ 61]\n",
            " [ 53]\n",
            " [ 72]\n",
            " [ 75]\n",
            " [ 50]\n",
            " [ 77]\n",
            " [100]\n",
            " [ 81]\n",
            " [100]\n",
            " [ 87]\n",
            " [ 78]\n",
            " [ 48]\n",
            " [ 50]\n",
            " [ 44]\n",
            " [ 48]\n",
            " [ 43]\n",
            " [ 67]\n",
            " [ 78]\n",
            " [ 58]\n",
            " [ 91]\n",
            " [ 92]\n",
            " [ 78]\n",
            " [ 42]\n",
            " [ 85]\n",
            " [ 73]\n",
            " [ 83]\n",
            " [ 61]\n",
            " [ 58]\n",
            " [ 60]\n",
            " [ 55]\n",
            " [ 48]\n",
            " [ 62]\n",
            " [ 68]\n",
            " [ 59]\n",
            " [ 62]\n",
            " [ 48]\n",
            " [ 74]\n",
            " [ 63]\n",
            " [ 80]\n",
            " [ 79]\n",
            " [ 73]\n",
            " [ 79]\n",
            " [ 45]\n",
            " [ 67]\n",
            " [ 89]\n",
            " [ 77]\n",
            " [ 81]\n",
            " [ 88]\n",
            " [ 53]\n",
            " [ 68]\n",
            " [ 79]\n",
            " [ 77]\n",
            " [ 63]\n",
            " [ 73]\n",
            " [ 60]\n",
            " [ 67]\n",
            " [100]\n",
            " [ 79]\n",
            " [ 26]\n",
            " [ 51]\n",
            " [ 80]\n",
            " [ 57]\n",
            " [ 41]\n",
            " [ 78]\n",
            " [ 68]\n",
            " [ 49]\n",
            " [ 76]\n",
            " [ 41]\n",
            " [ 71]\n",
            " [ 77]\n",
            " [ 89]\n",
            " [ 86]\n",
            " [ 55]\n",
            " [ 80]\n",
            " [ 56]\n",
            " [ 74]\n",
            " [ 85]\n",
            " [ 80]\n",
            " [ 73]\n",
            " [ 74]\n",
            " [ 86]\n",
            " [ 56]\n",
            " [ 53]\n",
            " [ 44]\n",
            " [ 41]\n",
            " [ 59]\n",
            " [ 71]\n",
            " [ 81]\n",
            " [ 74]\n",
            " [ 78]\n",
            " [ 67]\n",
            " [ 53]\n",
            " [ 56]\n",
            " [ 75]\n",
            " [ 82]\n",
            " [ 79]\n",
            " [ 99]\n",
            " [ 76]\n",
            " [ 59]\n",
            " [ 96]\n",
            " [ 75]\n",
            " [ 61]\n",
            " [ 56]\n",
            " [ 88]\n",
            " [ 65]\n",
            " [100]\n",
            " [ 79]\n",
            " [ 55]\n",
            " [ 61]\n",
            " [ 83]\n",
            " [ 74]\n",
            " [ 59]\n",
            " [ 54]\n",
            " [ 47]\n",
            " [ 82]\n",
            " [ 74]\n",
            " [ 59]\n",
            " [ 74]\n",
            " [ 84]\n",
            " [ 59]\n",
            " [ 43]\n",
            " [ 65]\n",
            " [ 61]\n",
            " [ 78]\n",
            " [ 84]\n",
            " [ 73]\n",
            " [ 73]\n",
            " [ 92]\n",
            " [ 63]\n",
            " [ 72]\n",
            " [ 61]\n",
            " [ 59]\n",
            " [ 70]\n",
            " [ 87]\n",
            " [ 78]\n",
            " [ 65]\n",
            " [ 73]\n",
            " [ 62]\n",
            " [ 69]\n",
            " [ 55]\n",
            " [ 73]\n",
            " [ 63]\n",
            " [ 67]\n",
            " [ 86]\n",
            " [ 78]\n",
            " [ 85]\n",
            " [ 83]\n",
            " [ 80]\n",
            " [ 60]\n",
            " [ 90]\n",
            " [ 56]\n",
            " [ 70]\n",
            " [ 55]\n",
            " [ 80]\n",
            " [ 82]\n",
            " [ 60]\n",
            " [ 78]\n",
            " [ 76]\n",
            " [ 94]\n",
            " [ 75]\n",
            " [ 68]\n",
            " [ 71]\n",
            " [ 85]\n",
            " [ 46]\n",
            " [ 58]\n",
            " [ 46]\n",
            " [ 84]\n",
            " [ 58]\n",
            " [ 57]\n",
            " [ 59]\n",
            " [ 77]\n",
            " [ 63]\n",
            " [ 68]\n",
            " [ 99]\n",
            " [ 48]\n",
            " [ 91]\n",
            " [ 57]\n",
            " [ 80]\n",
            " [ 46]\n",
            " [ 75]\n",
            " [ 59]\n",
            " [ 87]\n",
            " [ 82]\n",
            " [ 79]\n",
            " [ 66]\n",
            " [ 68]\n",
            " [ 66]\n",
            " [ 61]\n",
            " [ 66]\n",
            " [ 63]\n",
            " [ 72]\n",
            " [ 73]\n",
            " [ 77]\n",
            " [ 84]\n",
            " [ 83]\n",
            " [ 42]\n",
            " [ 72]\n",
            " [ 76]\n",
            " [ 76]\n",
            " [ 39]\n",
            " [ 74]\n",
            " [ 43]\n",
            " [ 63]\n",
            " [ 74]\n",
            " [ 52]\n",
            " [ 31]\n",
            " [ 65]\n",
            " [ 45]\n",
            " [ 87]\n",
            " [ 63]\n",
            " [ 51]\n",
            " [ 82]\n",
            " [ 86]\n",
            " [ 76]\n",
            " [ 27]\n",
            " [ 70]\n",
            " [ 79]\n",
            " [ 66]\n",
            " [ 61]\n",
            " [ 62]\n",
            " [ 47]\n",
            " [ 17]\n",
            " [ 65]\n",
            " [ 76]\n",
            " [ 75]\n",
            " [ 66]\n",
            " [ 59]\n",
            " [ 61]\n",
            " [ 93]\n",
            " [ 40]\n",
            " [ 66]\n",
            " [ 43]\n",
            " [ 71]\n",
            " [ 64]\n",
            " [ 55]\n",
            " [ 86]\n",
            " [ 65]\n",
            " [ 70]\n",
            " [ 65]\n",
            " [ 53]\n",
            " [ 49]\n",
            " [ 67]\n",
            " [ 76]\n",
            " [ 95]\n",
            " [ 76]\n",
            " [ 48]\n",
            " [ 60]\n",
            " [ 53]\n",
            " [ 69]\n",
            " [ 78]\n",
            " [ 62]\n",
            " [ 66]\n",
            " [ 51]\n",
            " [ 52]\n",
            " [ 46]\n",
            " [ 42]\n",
            " [ 77]\n",
            " [ 57]\n",
            " [100]\n",
            " [ 84]\n",
            " [ 68]\n",
            " [ 48]\n",
            " [ 72]\n",
            " [ 50]\n",
            " [ 72]\n",
            " [ 55]\n",
            " [ 72]\n",
            " [ 77]\n",
            " [ 56]\n",
            " [ 94]\n",
            " [ 67]\n",
            " [ 82]\n",
            " [ 75]\n",
            " [ 80]\n",
            " [ 60]\n",
            " [ 73]\n",
            " [ 74]\n",
            " [ 62]\n",
            " [ 53]\n",
            " [ 69]\n",
            " [ 75]\n",
            " [ 60]\n",
            " [ 58]\n",
            " [ 71]\n",
            " [ 87]\n",
            " [ 74]\n",
            " [ 87]\n",
            " [ 73]\n",
            " [ 78]\n",
            " [ 76]\n",
            " [ 74]\n",
            " [ 55]\n",
            " [ 94]\n",
            " [ 71]\n",
            " [ 76]\n",
            " [ 59]\n",
            " [ 91]\n",
            " [ 57]\n",
            " [ 83]\n",
            " [ 59]\n",
            " [ 93]\n",
            " [ 64]\n",
            " [ 58]\n",
            " [ 79]\n",
            " [ 96]\n",
            " [ 76]\n",
            " [ 64]\n",
            " [ 70]\n",
            " [ 80]\n",
            " [ 33]\n",
            " [ 95]\n",
            " [ 64]\n",
            " [ 92]\n",
            " [ 34]\n",
            " [ 72]\n",
            " [ 81]\n",
            " [ 57]\n",
            " [ 79]\n",
            " [ 84]\n",
            " [ 82]\n",
            " [ 54]\n",
            " [ 45]\n",
            " [ 54]\n",
            " [ 62]\n",
            " [ 49]\n",
            " [ 74]\n",
            " [ 59]\n",
            " [ 63]\n",
            " [ 83]\n",
            " [ 62]\n",
            " [ 72]\n",
            " [ 72]\n",
            " [ 65]\n",
            " [ 65]\n",
            " [ 54]\n",
            " [ 78]\n",
            " [ 82]\n",
            " [ 85]\n",
            " [ 74]\n",
            " [ 83]\n",
            " [ 71]\n",
            " [ 83]\n",
            " [ 77]\n",
            " [ 66]\n",
            " [ 75]\n",
            " [ 52]\n",
            " [ 68]\n",
            " [ 84]\n",
            " [ 67]\n",
            " [ 70]\n",
            " [ 41]\n",
            " [ 91]\n",
            " [ 46]\n",
            " [ 58]\n",
            " [ 67]\n",
            " [ 70]\n",
            " [ 83]\n",
            " [ 64]\n",
            " [100]\n",
            " [ 49]\n",
            " [ 77]\n",
            " [ 57]\n",
            " [ 67]\n",
            " [ 80]\n",
            " [ 74]\n",
            " [ 41]\n",
            " [ 67]\n",
            " [ 59]\n",
            " [ 86]\n",
            " [ 88]\n",
            " [ 57]\n",
            " [ 80]\n",
            " [ 58]\n",
            " [ 52]\n",
            " [ 31]\n",
            " [ 84]\n",
            " [ 97]\n",
            " [ 71]\n",
            " [ 62]\n",
            " [ 58]\n",
            " [ 71]\n",
            " [ 41]\n",
            " [ 66]\n",
            " [100]\n",
            " [ 51]\n",
            " [ 35]\n",
            " [ 81]\n",
            " [ 94]\n",
            " [ 72]\n",
            " [ 38]\n",
            " [ 82]\n",
            " [ 79]\n",
            " [ 55]\n",
            " [ 75]\n",
            " [ 90]\n",
            " [ 95]\n",
            " [ 65]\n",
            " [ 39]\n",
            " [ 85]\n",
            " [ 86]\n",
            " [ 54]\n",
            " [ 93]\n",
            " [ 69]\n",
            " [ 84]\n",
            " [ 78]\n",
            " [ 58]\n",
            " [ 73]\n",
            " [ 60]\n",
            " [ 44]\n",
            " [ 67]\n",
            " [ 69]\n",
            " [ 55]\n",
            " [ 59]\n",
            " [ 88]\n",
            " [ 42]\n",
            " [ 78]\n",
            " [ 84]\n",
            " [ 68]\n",
            " [ 66]\n",
            " [ 51]\n",
            " [ 43]\n",
            " [ 38]\n",
            " [ 69]\n",
            " [ 90]\n",
            " [ 73]\n",
            " [ 67]\n",
            " [ 57]\n",
            " [ 81]\n",
            " [ 63]\n",
            " [ 80]\n",
            " [ 78]\n",
            " [ 65]\n",
            " [ 74]\n",
            " [ 80]\n",
            " [ 60]\n",
            " [ 60]\n",
            " [ 63]\n",
            " [ 64]\n",
            " [ 72]\n",
            " [ 51]\n",
            " [ 71]\n",
            " [ 63]\n",
            " [ 82]\n",
            " [ 76]\n",
            " [ 39]\n",
            " [ 79]\n",
            " [ 48]\n",
            " [ 70]\n",
            " [ 90]\n",
            " [ 73]\n",
            " [ 58]\n",
            " [100]\n",
            " [ 80]\n",
            " [ 75]\n",
            " [ 72]\n",
            " [ 79]\n",
            " [ 52]\n",
            " [ 56]\n",
            " [ 65]\n",
            " [ 45]\n",
            " [ 59]\n",
            " [ 61]\n",
            " [ 47]\n",
            " [ 62]\n",
            " [ 83]\n",
            " [ 90]\n",
            " [ 76]\n",
            " [ 72]\n",
            " [ 69]\n",
            " [ 57]\n",
            " [ 56]\n",
            " [ 40]\n",
            " [ 79]\n",
            " [ 48]\n",
            " [ 57]\n",
            " [ 47]\n",
            " [ 78]\n",
            " [ 45]\n",
            " [ 74]\n",
            " [ 69]\n",
            " [ 59]\n",
            " [ 85]\n",
            " [ 45]\n",
            " [ 54]\n",
            " [ 72]\n",
            " [ 74]\n",
            " [ 75]\n",
            " [ 55]\n",
            " [ 49]\n",
            " [ 53]\n",
            " [ 83]\n",
            " [ 22]\n",
            " [100]\n",
            " [ 67]\n",
            " [ 83]\n",
            " [ 46]\n",
            " [ 43]\n",
            " [ 74]\n",
            " [ 64]\n",
            " [ 35]\n",
            " [ 67]\n",
            " [ 87]\n",
            " [ 77]\n",
            " [ 91]\n",
            " [ 74]\n",
            " [ 96]\n",
            " [ 82]\n",
            " [ 78]\n",
            " [ 73]\n",
            " [ 52]\n",
            " [ 91]\n",
            " [ 66]\n",
            " [ 67]\n",
            " [ 71]\n",
            " [ 74]\n",
            " [ 71]\n",
            " [ 61]\n",
            " [ 47]\n",
            " [ 76]\n",
            " [ 85]\n",
            " [ 93]\n",
            " [ 41]\n",
            " [ 81]\n",
            " [ 86]\n",
            " [ 53]\n",
            " [ 91]\n",
            " [ 68]\n",
            " [ 96]\n",
            " [ 48]\n",
            " [ 71]\n",
            " [ 75]\n",
            " [ 72]\n",
            " [ 71]\n",
            " [ 62]\n",
            " [ 67]\n",
            " [ 53]\n",
            " [ 74]\n",
            " [ 63]\n",
            " [ 82]\n",
            " [ 57]\n",
            " [ 69]\n",
            " [ 52]\n",
            " [ 91]\n",
            " [ 73]\n",
            " [ 73]\n",
            " [ 75]\n",
            " [ 36]\n",
            " [ 71]\n",
            " [ 62]\n",
            " [100]\n",
            " [ 50]\n",
            " [ 74]\n",
            " [ 60]\n",
            " [ 75]\n",
            " [ 83]\n",
            " [ 83]\n",
            " [100]\n",
            " [ 67]\n",
            " [ 71]\n",
            " [ 77]\n",
            " [ 67]\n",
            " [ 95]\n",
            " [ 52]\n",
            " [ 71]\n",
            " [ 74]\n",
            " [ 60]\n",
            " [ 67]\n",
            " [ 79]\n",
            " [ 75]\n",
            " [ 95]\n",
            " [ 69]\n",
            " [ 80]\n",
            " [ 48]\n",
            " [ 61]\n",
            " [ 82]\n",
            " [ 39]\n",
            " [ 70]\n",
            " [ 70]\n",
            " [ 69]\n",
            " [ 32]\n",
            " [ 79]\n",
            " [ 53]\n",
            " [ 59]\n",
            " [ 83]\n",
            " [100]\n",
            " [ 80]\n",
            " [ 80]\n",
            " [ 82]\n",
            " [ 56]\n",
            " [ 83]\n",
            " [ 85]\n",
            " [ 88]\n",
            " [ 81]\n",
            " [ 95]\n",
            " [ 63]\n",
            " [ 70]\n",
            " [ 89]\n",
            " [ 59]\n",
            " [ 56]\n",
            " [ 62]\n",
            " [ 95]\n",
            " [ 63]\n",
            " [ 82]\n",
            " [ 69]\n",
            " [ 58]\n",
            " [ 74]\n",
            " [ 66]\n",
            " [ 82]\n",
            " [ 94]\n",
            " [ 70]\n",
            " [ 78]\n",
            " [ 63]\n",
            " [ 91]\n",
            " [ 70]\n",
            " [ 62]\n",
            " [ 79]\n",
            " [ 65]\n",
            " [ 74]\n",
            " [ 56]\n",
            " [ 65]\n",
            " [100]\n",
            " [ 70]\n",
            " [ 66]\n",
            " [ 54]\n",
            " [ 72]\n",
            " [ 90]\n",
            " [ 56]\n",
            " [ 65]\n",
            " [ 50]\n",
            " [ 95]\n",
            " [ 38]\n",
            " [ 76]\n",
            " [ 84]\n",
            " [ 76]\n",
            " [ 55]\n",
            " [ 85]\n",
            " [ 70]\n",
            " [ 73]\n",
            " [ 80]\n",
            " [ 83]\n",
            " [ 53]\n",
            " [ 67]\n",
            " [100]\n",
            " [ 67]\n",
            " [ 44]\n",
            " [ 96]\n",
            " [ 48]\n",
            " [ 77]\n",
            " [100]\n",
            " [ 40]\n",
            " [ 91]\n",
            " [ 55]\n",
            " [ 41]\n",
            " [ 25]\n",
            " [ 63]\n",
            " [ 59]\n",
            " [ 63]\n",
            " [ 77]\n",
            " [ 46]\n",
            " [ 49]\n",
            " [ 46]\n",
            " [ 93]\n",
            " [ 39]\n",
            " [ 58]\n",
            " [ 87]\n",
            " [ 57]\n",
            " [ 77]\n",
            " [100]\n",
            " [ 65]\n",
            " [ 34]\n",
            " [ 87]\n",
            " [ 81]\n",
            " [ 63]\n",
            " [ 69]\n",
            " [ 74]\n",
            " [ 70]\n",
            " [ 93]\n",
            " [ 63]\n",
            " [ 81]\n",
            " [ 81]\n",
            " [ 63]\n",
            " [ 87]\n",
            " [ 76]\n",
            " [ 54]\n",
            " [ 89]\n",
            " [ 63]\n",
            " [ 76]\n",
            " [ 79]\n",
            " [ 75]\n",
            " [ 50]\n",
            " [ 36]\n",
            " [ 82]\n",
            " [ 83]\n",
            " [ 85]\n",
            " [ 82]\n",
            " [ 41]\n",
            " [ 82]\n",
            " [ 45]\n",
            " [ 57]\n",
            " [ 88]\n",
            " [ 81]\n",
            " [ 98]\n",
            " [ 61]\n",
            " [ 95]\n",
            " [ 84]\n",
            " [ 71]\n",
            " [ 52]\n",
            " [ 71]\n",
            " [ 90]\n",
            " [ 75]\n",
            " [ 62]\n",
            " [ 63]\n",
            " [ 86]\n",
            " [ 70]\n",
            " [ 77]\n",
            " [ 68]\n",
            " [ 80]\n",
            " [ 67]\n",
            " [ 67]\n",
            " [ 89]\n",
            " [ 60]\n",
            " [ 79]\n",
            " [ 80]\n",
            " [ 78]\n",
            " [ 70]\n",
            " [ 72]\n",
            " [ 43]\n",
            " [ 14]\n",
            " [ 54]\n",
            " [ 92]\n",
            " [ 71]\n",
            " [ 65]\n",
            " [ 58]\n",
            " [ 56]\n",
            " [ 67]\n",
            " [ 64]\n",
            " [ 81]\n",
            " [ 55]\n",
            " [ 45]\n",
            " [ 86]\n",
            " [ 52]\n",
            " [ 75]\n",
            " [ 81]\n",
            " [ 62]\n",
            " [ 42]\n",
            " [ 21]\n",
            " [ 72]\n",
            " [ 55]\n",
            " [ 66]\n",
            " [ 69]\n",
            " [ 86]\n",
            " [ 67]\n",
            " [ 78]\n",
            " [ 85]\n",
            " [ 66]\n",
            " [ 47]\n",
            " [100]\n",
            " [ 63]\n",
            " [ 62]\n",
            " [ 61]\n",
            " [ 69]\n",
            " [ 57]\n",
            " [ 76]\n",
            " [ 52]\n",
            " [ 47]\n",
            " [ 51]\n",
            " [ 61]\n",
            " [ 45]\n",
            " [ 59]\n",
            " [ 81]\n",
            " [ 65]\n",
            " [ 53]\n",
            " [ 61]\n",
            " [ 90]\n",
            " [ 74]\n",
            " [ 62]\n",
            " [ 67]\n",
            " [ 50]\n",
            " [ 84]\n",
            " [ 70]\n",
            " [ 52]\n",
            " [ 92]\n",
            " [ 65]\n",
            " [ 65]\n",
            " [ 67]\n",
            " [ 72]\n",
            " [ 66]\n",
            " [ 62]\n",
            " [ 99]\n",
            " [ 62]\n",
            " [ 53]\n",
            " [ 57]\n",
            " [ 78]\n",
            " [ 56]\n",
            " [ 87]\n",
            " [ 79]\n",
            " [ 63]\n",
            " [ 87]\n",
            " [ 86]\n",
            " [ 75]\n",
            " [ 70]\n",
            " [ 60]\n",
            " [ 49]\n",
            " [ 41]\n",
            " [ 78]\n",
            " [ 58]\n",
            " [ 75]\n",
            " [ 89]\n",
            " [ 34]\n",
            " [ 60]\n",
            " [ 80]\n",
            " [ 85]\n",
            " [ 73]\n",
            " [ 58]\n",
            " [ 69]\n",
            " [ 74]\n",
            " [ 52]\n",
            " [ 58]\n",
            " [ 79]\n",
            " [ 86]\n",
            " [ 61]\n",
            " [ 68]\n",
            " [ 67]\n",
            " [ 48]\n",
            " [ 65]\n",
            " [ 73]\n",
            " [ 57]\n",
            " [ 73]\n",
            " [ 57]\n",
            " [ 80]\n",
            " [ 85]\n",
            " [ 81]\n",
            " [ 61]\n",
            " [ 69]\n",
            " [100]\n",
            " [ 99]\n",
            " [ 92]\n",
            " [ 72]\n",
            " [ 57]\n",
            " [ 44]\n",
            " [ 59]\n",
            " [ 62]\n",
            " [ 93]\n",
            " [ 64]\n",
            " [ 57]\n",
            " [ 72]\n",
            " [ 40]\n",
            " [ 85]\n",
            " [ 60]\n",
            " [ 83]\n",
            " [ 63]\n",
            " [ 74]\n",
            " [ 44]\n",
            " [ 61]\n",
            " [ 74]\n",
            " [ 68]\n",
            " [ 78]\n",
            " [ 50]\n",
            " [ 70]\n",
            " [ 68]\n",
            " [ 82]\n",
            " [ 46]\n",
            " [ 96]\n",
            " [100]\n",
            " [ 44]\n",
            " [ 41]\n",
            " [ 95]\n",
            " [ 79]\n",
            " [ 67]\n",
            " [ 52]\n",
            " [ 87]\n",
            " [ 75]\n",
            " [ 61]\n",
            " [ 42]\n",
            " [ 60]\n",
            " [ 57]\n",
            " [ 64]\n",
            " [ 52]\n",
            " [ 68]\n",
            " [ 58]\n",
            " [ 93]\n",
            " [ 75]\n",
            " [ 77]\n",
            " [ 66]\n",
            " [ 63]\n",
            " [ 90]\n",
            " [ 43]\n",
            " [ 65]\n",
            " [ 95]\n",
            " [ 86]\n",
            " [ 31]\n",
            " [ 95]\n",
            " [ 52]\n",
            " [ 63]\n",
            " [ 87]\n",
            " [ 70]\n",
            " [ 59]\n",
            " [ 84]\n",
            " [ 79]\n",
            " [ 77]\n",
            " [ 75]\n",
            " [ 66]\n",
            " [ 69]\n",
            " [ 85]\n",
            " [ 63]\n",
            " [ 50]\n",
            " [ 58]\n",
            " [ 80]\n",
            " [ 47]\n",
            " [ 55]\n",
            " [ 61]\n",
            " [ 87]\n",
            " [ 77]\n",
            " [ 54]\n",
            " [ 66]\n",
            " [ 68]\n",
            " [ 54]\n",
            " [ 69]\n",
            " [ 74]\n",
            " [ 81]\n",
            " [ 72]\n",
            " [ 61]\n",
            " [ 76]\n",
            " [ 63]\n",
            " [ 64]\n",
            " [ 73]\n",
            " [ 62]\n",
            " [ 92]\n",
            " [ 69]\n",
            " [ 70]\n",
            " [ 65]\n",
            " [ 53]\n",
            " [ 74]\n",
            " [ 61]\n",
            " [ 80]\n",
            " [ 85]\n",
            " [ 62]\n",
            " [ 80]\n",
            " [ 83]\n",
            " [ 56]\n",
            " [ 76]\n",
            " [ 52]\n",
            " [ 51]\n",
            " [ 74]\n",
            " [ 57]\n",
            " [ 63]\n",
            " [ 61]\n",
            " [ 87]\n",
            " [ 60]\n",
            " [ 54]\n",
            " [ 89]\n",
            " [ 67]\n",
            " [ 56]\n",
            " [ 70]\n",
            " [ 90]\n",
            " [ 94]\n",
            " [ 78]\n",
            " [ 72]]\n",
            "\n",
            "Matrix W (Weights):\n",
            "[[0.16256911]\n",
            " [0.65015846]]\n",
            "\n",
            "Matrix X (Features):\n",
            "[[48 62 79 ... 89 83 66]\n",
            " [68 81 80 ... 87 82 66]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#To - Do - 3:\n",
        "1. Split the dataset into training and test sets.\n",
        "2. You can use an 80-20 or 70-30 split, with 80% (or 70%) of the data used for training and the rest\n",
        "for testing."
      ],
      "metadata": {
        "id": "jMXwtwF0ciD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare features (X) and labels (Y)\n",
        "X = data[['Math', 'Reading']]\n",
        "Y = data['Writing']\n",
        "\n",
        "# Split the dataset into training and test sets (80-20 split)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Display the sizes of the training and test sets\n",
        "print(\"Training set size:\")\n",
        "print(X_train.shape, Y_train.shape)\n",
        "\n",
        "print(\"\\nTest set size:\")\n",
        "print(X_test.shape, Y_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTLbkjQedE7V",
        "outputId": "2f771971-71dd-4102-e393-29db45a3d6e2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size:\n",
            "(800, 2) (800,)\n",
            "\n",
            "Test set size:\n",
            "(200, 2) (200,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Implementation of the Cost Function\n",
        "#Define the cost function\n",
        "def cost_function(X, Y, W):\n",
        "\"\"\" Parameters:\n",
        "This function finds the Mean Square Error.\n",
        "Input parameters:\n",
        "X: Feature Matrix\n",
        "Y: Target Matrix\n",
        "W: Weight Matrix\n",
        "Output Parameters:\n",
        "cost: accumulated mean square error.\n",
        "\"\"\"\n",
        "# Your code here:\n",
        "return cost"
      ],
      "metadata": {
        "id": "0pem_E1XeGjy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the cost function\n",
        "def cost_function(X, Y, W):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    This function finds the Mean Square Error.\n",
        "\n",
        "    Input parameters:\n",
        "    X: Feature Matrix (shape: d x n)\n",
        "    Y: Target Matrix (shape: n x 1)\n",
        "    W: Weight Matrix (shape: d x 1)\n",
        "\n",
        "    Output Parameters:\n",
        "    cost: accumulated mean square error.\n",
        "    \"\"\"\n",
        "    # Calculate predicted values\n",
        "    Y_pred = np.dot(W.T, X)  # W^T * X, shape: 1 x n\n",
        "\n",
        "    # Calculate the error\n",
        "    error = Y_pred - Y.flatten()  # Flatten Y to match dimensions\n",
        "\n",
        "    # Calculate Mean Squared Error\n",
        "    cost = (1 / (2 * len(Y))) * np.sum(error ** 2)  # MSE formula\n",
        "\n",
        "    return cost"
      ],
      "metadata": {
        "id": "Vi0OHi4PePXZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W = np.random.rand(2, 1)  # Example weight initialization\n",
        "\n",
        "# Calculate the cost using the training set\n",
        "cost = cost_function(X_train.T, Y_train.values.reshape(-1, 1), W)\n",
        "print(f\"Cost: {cost}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8681Ag2eb5J",
        "outputId": "b1e117ef-dd3d-4e17-eefe-9673db7ad6da"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost: 13.529973103735724\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.1.3 Step -3- Gradient Descent for Simple Linear Regression:\n",
        "Implement your code here or write your own:\n",
        "\n",
        "Gradient Descent from Scratch:"
      ],
      "metadata": {
        "id": "TB4RJti3ehi1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the cost function\n",
        "def cost_function(X, Y, W):\n",
        "    Y_pred = np.dot(W.T, X)  # Predicted values\n",
        "    error = Y_pred - Y.flatten()  # Flatten Y to match dimensions\n",
        "    cost = (1 / (2 * len(Y))) * np.sum(error ** 2)  # MSE formula\n",
        "    return cost\n",
        "\n",
        "# Define the gradient descent function\n",
        "def gradient_descent(X, Y, W, alpha, iterations):\n",
        "    \"\"\"\n",
        "    Perform gradient descent to optimize the parameters of a linear regression model.\n",
        "\n",
        "    Parameters:\n",
        "    X (numpy.ndarray): Feature matrix (m x n).\n",
        "    Y (numpy.ndarray): Target vector (m x 1).\n",
        "    W (numpy.ndarray): Initial guess for parameters (n x 1).\n",
        "    alpha (float): Learning rate.\n",
        "    iterations (int): Number of iterations for gradient descent.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing the final optimized parameters (W_update) and the history of cost values.\n",
        "        W_update (numpy.ndarray): Updated parameters (n x 1).\n",
        "        cost_history (list): History of cost values over iterations.\n",
        "    \"\"\"\n",
        "    # Initialize cost history\n",
        "    cost_history = [0] * iterations\n",
        "    # Number of samples\n",
        "    m = len(Y)\n",
        "\n",
        "    for iteration in range(iterations):\n",
        "        # Step 1: Hypothesis Values\n",
        "        Y_pred = np.dot(W.T, X)  # Predicted values\n",
        "\n",
        "        # Step 2: Difference between Hypothesis and Actual Y\n",
        "        loss = Y_pred - Y.flatten()  # Flatten Y to match dimensions\n",
        "\n",
        "        # Step 3: Gradient Calculation\n",
        "        dw = (1 / m) * np.dot(X, loss.reshape(-1, 1))  # Gradient calculation\n",
        "\n",
        "        # Step 4: Updating Values of W using Gradient\n",
        "        W_update = W - alpha * dw  # Update weights\n",
        "\n",
        "        # Step 5: New Cost Value\n",
        "        cost = cost_function(X, Y, W_update)\n",
        "        cost_history[iteration] = cost\n",
        "\n",
        "        # Update W for next iteration\n",
        "        W = W_update\n",
        "\n",
        "    return W_update, cost_history\n",
        "\n",
        "\n",
        "X = data[['Math', 'Reading']].values.T  # Transpose to get shape (d x n)\n",
        "Y = data['Writing'].values.reshape(-1, 1)  # Reshape to column vector\n",
        "\n",
        "# Initialize weights randomly\n",
        "W_initial = np.random.rand(2, 1)  # Two features\n",
        "\n",
        "# Set learning rate and number of iterations\n",
        "alpha = 0.01\n",
        "iterations = 1000\n",
        "\n",
        "# Perform gradient descent\n",
        "W_final, cost_history = gradient_descent(X, Y, W_initial, alpha, iterations)\n",
        "\n",
        "print(\"Final Weights:\")\n",
        "print(W_final)\n",
        "print(\"Cost History:\")\n",
        "print(cost_history)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xl11nZJUfLvS",
        "outputId": "584976dc-7d4d-4079-c724-ff23613202f4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Weights:\n",
            "[[nan]\n",
            " [nan]]\n",
            "Cost History:\n",
            "[2695717.3947992623, 25409913292.70699, 239515737026873.16, 2.2576931943055895e+18, 2.1281184371789795e+22, 2.0059803050671285e+26, 1.8908519911379268e+30, 1.7823311840893737e+34, 1.6800386622887788e+38, 1.5836169685978723e+42, 1.492729161253102e+46, 1.4070576364360786e+50, 1.3263030184196913e+54, 1.2501831134115705e+58, 1.1784319234391348e+62, 1.1107987168302829e+66, 1.0470471520415596e+70, 9.8695445177206e+73, 9.303106244770372e+77, 8.769177305608224e+81, 8.265891906847995e+85, 7.791491337733439e+89, 7.344317824393702e+93, 6.92280873681851e+97, 6.525491128310095e+101, 6.15097658832953e+105, 5.797956390751531e+109, 5.465196920573219e+113, 5.151535363095663e+117, 4.855875640514281e+121, 4.577184581718674e+125, 4.314488311917371e+129, 4.0668688494712233e+133, 3.833460898043128e+137, 3.613448822854555e+141, 3.4060638004822727e+145, 3.2105811322355946e+149, 3.0263177117256796e+153, 2.852629637777516e+157, 2.6889099643429336e+161, 2.5345865795518377e+165, 2.3891202064901042e+169, 2.2520025187178953e+173, 2.1227543639432008e+177, 2.000924089643244e+181, 1.88608596478277e+185, 1.7778386921139054e+189, 1.6758040058589364e+193, 1.5796253498756307e+197, 1.4889666316860129e+201, 1.40351104801461e+205, 1.3229599777320329e+209, 1.2470319383353517e+213, 1.1754616023187126e+217, 1.10799886999695e+221, 1.0444079955422078e+225, 9.844667631795457e+228, 9.279657106627782e+232, 8.747073973170032e+236, 8.245057140900398e+240, 7.771852332017679e+244, 7.325805951187438e+248, 6.905359307119068e+252, 6.509043165781213e+256, 6.135472616222053e+260, 5.783342231053193e+264, 5.4514215046857325e+268, 5.138550553377537e+272, 4.843636062065754e+276, 4.565647463820946e+280, 4.3036133385720214e+284, 4.0566180185175496e+288, 3.8237983883611534e+292, 3.60434086918951e+296, 3.3974785754532715e+300, 3.2024886351161273e+304, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-f0842c8f528a>:5: RuntimeWarning: overflow encountered in square\n",
            "  cost = (1 / (2 * len(Y))) * np.sum(error ** 2)  # MSE formula\n",
            "<ipython-input-9-f0842c8f528a>:41: RuntimeWarning: invalid value encountered in subtract\n",
            "  W_update = W - alpha * dw  # Update weights\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test Code for Gradient Descent function:**"
      ],
      "metadata": {
        "id": "0BarLts6fdfO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the cost function\n",
        "def cost_function(X, Y, W):\n",
        "    Y_pred = np.dot(X, W)  # Predicted values\n",
        "    error = Y_pred - Y  # Calculate error\n",
        "    cost = (1 / (2 * len(Y))) * np.sum(error ** 2)  # MSE formula\n",
        "    return cost\n",
        "\n",
        "# Define the gradient descent function\n",
        "def gradient_descent(X, Y, W, alpha, iterations):\n",
        "    \"\"\"\n",
        "    Perform gradient descent to optimize the parameters of a linear regression model.\n",
        "\n",
        "    Parameters:\n",
        "    X (numpy.ndarray): Feature matrix (m x n).\n",
        "    Y (numpy.ndarray): Target vector (m x 1).\n",
        "    W (numpy.ndarray): Initial guess for parameters (n x 1).\n",
        "    alpha (float): Learning rate.\n",
        "    iterations (int): Number of iterations for gradient descent.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing the final optimized parameters (W_update) and the history of cost values.\n",
        "        W_update (numpy.ndarray): Updated parameters (n x 1).\n",
        "        cost_history (list): History of cost values over iterations.\n",
        "    \"\"\"\n",
        "    # Initialize cost history\n",
        "    cost_history = [0] * iterations\n",
        "    # Number of samples\n",
        "    m = len(Y)\n",
        "\n",
        "    for iteration in range(iterations):\n",
        "        # Step 1: Hypothesis Values\n",
        "        Y_pred = np.dot(X, W)  # Predicted values\n",
        "\n",
        "        # Step 2: Difference between Hypothesis and Actual Y\n",
        "        loss = Y_pred - Y  # Calculate loss\n",
        "\n",
        "        # Step 3: Gradient Calculation\n",
        "        dw = (1 / m) * np.dot(X.T, loss)  # Gradient calculation\n",
        "\n",
        "        # Step 4: Updating Values of W using Gradient\n",
        "        W_update = W - alpha * dw  # Update weights\n",
        "\n",
        "        # Step 5: New Cost Value\n",
        "        cost = cost_function(X, Y, W_update)\n",
        "        cost_history[iteration] = cost\n",
        "\n",
        "        # Update W for next iteration\n",
        "        W = W_update\n",
        "\n",
        "    return W_update, cost_history\n",
        "\n",
        "# Generate random test data\n",
        "np.random.seed(0)  # For reproducibility\n",
        "X = np.random.rand(100, 3)  # 100 samples, 3 features\n",
        "Y = np.random.rand(100)      # Target values\n",
        "W = np.random.rand(3)        # Initial guess for parameters\n",
        "\n",
        "# Set hyperparameters\n",
        "alpha = 0.01\n",
        "iterations = 1000\n",
        "\n",
        "# Test the gradient_descent function\n",
        "final_params, cost_history = gradient_descent(X, Y.reshape(-1, 1), W.reshape(-1, 1), alpha, iterations)\n",
        "\n",
        "# Print the final parameters and cost history\n",
        "print(\"Final Parameters:\", final_params.flatten())\n",
        "print(\"Cost History:\", cost_history)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-Gr4Abkfiai",
        "outputId": "c93fd16b-254f-40a1-f703-295cdb6d3748"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Parameters: [0.20551667 0.54295081 0.10388027]\n",
            "Cost History: [0.10711197094660153, 0.10634880599939901, 0.10559826315680618, 0.10486012948320558, 0.1041341956428534, 0.10342025583900626, 0.1027181077540776, 0.1020275524908062, 0.10134839451441931, 0.1006804415957737, 0.1000235047554587, 0.09937739820884377, 0.09874193931205609, 0.09811694850887098, 0.09750224927850094, 0.0968976680842672, 0.09630303432313951, 0.09571818027612913, 0.09514294105952065, 0.09457715457692842, 0.09402066147216397, 0.09347330508290017, 0.09293493139511913, 0.09240538899833017, 0.09188452904154543, 0.0913722051899995, 0.09086827358260123, 0.09037259279010502, 0.08988502377398919, 0.08940542984603007, 0.08893367662855953, 0.08846963201539432, 0.08801316613342668, 0.08756415130486386, 0.08712246201010665, 0.08668797485125508, 0.08626056851623207, 0.08584012374351278, 0.08542652328745133, 0.08501965188419301, 0.0846193962181636, 0.08422564488912489, 0.08383828837978763, 0.08345721902397185, 0.08308233097530582, 0.08271352017645425, 0.08235068432886682, 0.08199372286303817, 0.08164253690927113, 0.08129702926893387, 0.08095710438620353, 0.08062266832028739, 0.08029362871811391, 0.07996989478748553, 0.0796513772706855, 0.07933798841853089, 0.07902964196486459, 0.07872625310147845, 0.07842773845346054, 0.07813401605495938, 0.0778450053253578, 0.0775606270458499, 0.07728080333641404, 0.07700545763317514, 0.07673451466614989, 0.07646790043736812, 0.07620554219936448, 0.07594736843403344, 0.07569330883184205, 0.07544329427139428, 0.07519725679934074, 0.07495512961062821, 0.07471684702908327, 0.07448234448832412, 0.0742515585129952, 0.07402442670031911, 0.0738008877019607, 0.07358088120619749, 0.0733643479203919, 0.07315122955375959, 0.07294146880042966, 0.07273500932279067, 0.07253179573511871, 0.07233177358748233, 0.0721348893499193, 0.07194109039688139, 0.07175032499194182, 0.07156254227276149, 0.07137769223630935, 0.07119572572433286, 0.07101659440907385, 0.07084025077922623, 0.070666648126131, 0.07049574053020462, 0.07032748284759716, 0.07016183069707572, 0.0699987404471299, 0.06983816920329523, 0.06968007479569092, 0.06952441576676843, 0.06937115135926715, 0.06922024150437375, 0.06907164681008185, 0.06892532854974835, 0.0687812486508435, 0.06863936968389095, 0.06849965485159508, 0.06836206797815195, 0.06822657349874123, 0.06809313644919561, 0.067961722455845, 0.06783229772553254, 0.06770482903579932, 0.06757928372523506, 0.06745562968399212, 0.06733383534445969, 0.06721386967209597, 0.06709570215641501, 0.06697930280212627, 0.06686464212042395, 0.06675169112042348, 0.0666404213007429, 0.06653080464122665, 0.06642281359480932, 0.06631642107951677, 0.06621160047060279, 0.06610832559281864, 0.06600657071281309, 0.0659063105316614, 0.06580752017752023, 0.06571017519840698, 0.06561425155510119, 0.06551972561416586, 0.06542657414108709, 0.06533477429352925, 0.06524430361470467, 0.06515514002685512, 0.06506726182484374, 0.06498064766985515, 0.06489527658320228, 0.06481112794023773, 0.06472818146436811, 0.0646464172211699, 0.06456581561260431, 0.06448635737133043, 0.0644080235551142, 0.06433079554133217, 0.06425465502156798, 0.06417958399630046, 0.06410556476968135, 0.06403257994440141, 0.0639606124166433, 0.06388964537111992, 0.06381966227619645, 0.06375064687909507, 0.06368258320118077, 0.06361545553332655, 0.06354924843135755, 0.06348394671157162, 0.06341953544633615, 0.06335599995975896, 0.06329332582343267, 0.06323149885225086, 0.06317050510029515, 0.06311033085679153, 0.06305096264213547, 0.06299238720398384, 0.0629345915134133, 0.06287756276114324, 0.06282128835382297, 0.0627657559103815, 0.06271095325843898, 0.06265686843077901, 0.06260348966188053, 0.06255080538450809, 0.06249880422636036, 0.06244747500677472, 0.06239680673348793, 0.06234678859945137, 0.06229740997970036, 0.0622486604282762, 0.06220052967520031, 0.062153007623499706, 0.062106084346282515, 0.062059750083863094, 0.06201399524093575, 0.06196881038379625, 0.061924186237610215, 0.06188011368372787, 0.0618365837570441, 0.06179358764340313, 0.061751116677047156, 0.06170916233810801, 0.0616677162501414, 0.06162677017770278, 0.061586316023964055, 0.0615463458283708, 0.06150685176433905, 0.06146782613699094, 0.0614292613809287, 0.061391150058046254, 0.06135348485537795, 0.06131625858298352, 0.061279464171868706, 0.06124309467194143, 0.061207143250002184, 0.06117160318776841, 0.06113646787993252, 0.061101730832252524, 0.06106738565967507, 0.06103342608449018, 0.06099984593451716, 0.06096663914132128, 0.0609337997384604, 0.0609013218597616, 0.06086919973762659, 0.06083742770136588, 0.06080600017556133, 0.06077491167845612, 0.06074415682037193, 0.06071373030215326, 0.060683626913637524, 0.06065384153215141, 0.06062436912103256, 0.0605952047281761, 0.06056634348460599, 0.060537780603070336, 0.060509511376660545, 0.0604815311774538, 0.060453835455178496, 0.06042641973590228, 0.06039927962074216, 0.060372410784596583, 0.060345808974898815, 0.06031947001039151, 0.06029338977992186, 0.06026756424125725, 0.060241989419920934, 0.06021666140804729, 0.0601915763632565, 0.06016673050754826, 0.060142120126214255, 0.06011774156676883, 0.06009359123789796, 0.06006966560842588, 0.06004596120629915, 0.060022474617588105, 0.059999202485504784, 0.059976141509438, 0.05995328844400421, 0.05993064009811483, 0.05990819333405906, 0.059885945066602345, 0.059863892262100066, 0.059842031937626106, 0.059820361160116395, 0.05979887704552664, 0.05977757675800453, 0.05975645750907579, 0.05973551655684408, 0.0597147512052044, 0.05969415880306974, 0.05967373674361096, 0.05965348246350928, 0.05963339344222168, 0.059613467201258485, 0.059593701303473294, 0.05957409335236496, 0.05955464099139111, 0.05953534190329372, 0.059516193809435625, 0.0594971944691485, 0.0594783416790919, 0.05945963327262296, 0.0594410671191769, 0.05942264112365792, 0.05940435322584049, 0.059386201399780576, 0.059368183653237094, 0.059350298027102844, 0.05933254259484533, 0.05931491546195686, 0.05929741476541398, 0.0592800386731462, 0.05926278538351338, 0.05924565312479226, 0.05922864015467154, 0.059211744759755505, 0.059194965255076046, 0.05917829998361292, 0.05916174731582212, 0.059145305649172315, 0.059128973407688926, 0.059112749041506096, 0.05909663102642617, 0.05908061786348662, 0.059064708078534194, 0.05904890022180654, 0.05903319286752055, 0.05901758461346795, 0.05900207408061755, 0.058986659912724324, 0.05897134077594505, 0.058956115358460404, 0.05894098237010357, 0.05892594054199501, 0.05891098862618344, 0.05889612539529293, 0.05888134964217589, 0.05886666017957195, 0.058852055839772675, 0.05883753547429179, 0.058823097953541174, 0.058808742166512155, 0.05879446702046235, 0.058780271440607684, 0.05876615436981961, 0.05875211476832761, 0.05873815161342641, 0.05872426389918856, 0.058710450636181515, 0.05869671085118971, 0.058683043586941104, 0.058669447901838714, 0.05865592286969638, 0.05864246757947903, 0.05862908113504752, 0.05861576265490756, 0.058602511271963004, 0.05858932613327336, 0.058576206399815284, 0.05856315124624814, 0.05855015986068357, 0.05853723144445888, 0.05852436521191438, 0.05851156039017436, 0.0584988162189318, 0.05848613195023677, 0.05847350684828838, 0.058460940189230176, 0.05844843126094919, 0.05843597936287807, 0.05842358380580092, 0.05841124391166213, 0.05839895901337858, 0.05838672845465502, 0.05837455158980245, 0.05836242778355972, 0.05835035641091811, 0.05833833685694878, 0.05832636851663321, 0.05831445079469655, 0.05830258310544367, 0.05829076487259809, 0.05827899552914358, 0.05826727451716845, 0.058255601287712386, 0.0582439753006161, 0.058232396024373266, 0.05822086293598511, 0.058209375520817355, 0.058197933272459756, 0.05818653569258779, 0.05817518229082684, 0.058163872584618616, 0.05815260609908985, 0.058141382366923164, 0.058130200928230166, 0.058119061330426776, 0.058107963128110396, 0.05809690588293942, 0.058085889163514745, 0.058074912545263056, 0.058063975610322414, 0.058053077947429504, 0.05804221915180897, 0.05803139882506447, 0.05802061657507169, 0.0580098720158732, 0.05799916476757483, 0.057988494456244134, 0.057977860713810364, 0.057967263177966154, 0.05795670149207094, 0.05794617530505593, 0.05793568427133074, 0.057925228050691516, 0.057914806308230836, 0.057904418714248757, 0.057894064944165734, 0.05788374467843681, 0.05787345760246728, 0.057863203406529826, 0.05785298178568306, 0.05784279243969133, 0.057832635072946004, 0.05782250939438805, 0.0578124151174319, 0.05780235195989063, 0.057792319643902336, 0.05778231789585793, 0.0577723464463298, 0.05776240503000217, 0.05775249338560223, 0.05774261125583255, 0.05773275838730474, 0.05772293453047407, 0.05771313943957533, 0.0577033728725597, 0.057693634591032654, 0.057683924360193005, 0.05767424194877295, 0.05766458712897906, 0.05765495967643434, 0.057645359370121226, 0.05763578599232564, 0.057626239328581796, 0.05761671916761811, 0.05760722530130401, 0.05759775752459752, 0.057588315635493874, 0.057578899434974906, 0.05756950872695933, 0.05756014331825381, 0.05755080301850501, 0.057541487640152184, 0.05753219699838088, 0.057522930911077144, 0.057513689198782685, 0.05750447168465076, 0.05749527819440267, 0.057486108556285255, 0.05747696260102877, 0.05746784016180581, 0.05745874107419066, 0.05744966517611951, 0.05744061230785123, 0.05743158231192885, 0.05742257503314173, 0.057413590318488285, 0.05740462801713937, 0.057395687980402336, 0.05738677006168561, 0.05737787411646401, 0.057369000002244354, 0.05736014757853204, 0.05735131670679789, 0.057342507250445686, 0.05733371907478018, 0.05732495204697581, 0.057316206036045626, 0.05730748091281111, 0.057298776549872255, 0.05729009282157824, 0.05728142960399854, 0.05727278677489465, 0.05726416421369212, 0.05725556180145319, 0.05724697942084986, 0.0572384169561373, 0.05722987429312795, 0.05722135131916572, 0.05721284792310103, 0.05720436399526589, 0.0571958994274496, 0.057187454112874896, 0.05717902794617434, 0.05717062082336728, 0.057162232641836994, 0.05715386330030848, 0.05714551269882637, 0.05713718073873334, 0.05712886732264895, 0.05712057235444866, 0.05711229573924336, 0.05710403738335914, 0.0570957971943175, 0.05708757508081579, 0.057079370952708035, 0.057071184720986066, 0.05706301629776107, 0.057054865596245175, 0.057046732530733744, 0.05703861701658757, 0.05703051897021566, 0.05702243830905818, 0.057014374951569684, 0.057006328817202634, 0.05699829982639134, 0.05699028790053585, 0.05698229296198646, 0.05697431493402824, 0.05696635374086599, 0.05695840930760929, 0.056950481560257914, 0.0569425704256875, 0.0569346758316353, 0.05692679770668645, 0.05691893598026014, 0.05691109058259633, 0.05690326144474244, 0.05689544849854041, 0.056887651676613984, 0.05687987091235604, 0.056872106139916355, 0.05686435729418944, 0.05685662431080263, 0.0568489071261043, 0.05684120567715239, 0.056833519901703065, 0.05682584973819958, 0.05681819512576124, 0.05681055600417275, 0.056802932313873525, 0.056795323995947306, 0.056787730992111936, 0.05678015324470926, 0.05677259069669528, 0.05676504329163035, 0.05675751097366966, 0.05674999368755382, 0.05674249137859953, 0.05673500399269066, 0.05672753147626912, 0.056720073776326194, 0.05671263084039382, 0.056705202616536096, 0.05669778905334098, 0.05669039009991206, 0.05668300570586036, 0.05667563582129657, 0.056668280396823104, 0.05666093938352648, 0.05665361273296975, 0.056646300397185066, 0.05663900232866641, 0.05663171848036241, 0.05662444880566923, 0.05661719325842369, 0.056609951792896414, 0.05660272436378514, 0.05659551092620811, 0.056588311435697584, 0.05658112584819342, 0.05657395412003692, 0.05656679620796451, 0.05655965206910184, 0.05655252166095763, 0.05654540494141801, 0.056538301868740586, 0.05653121240154893, 0.056524136498826864, 0.056517074119913024, 0.05651002522449555, 0.05650298977260663, 0.05649596772461748, 0.056488959041233064, 0.05648196368348717, 0.05647498161273735, 0.0564680127906601, 0.056461057179246134, 0.05645411474079556, 0.05644718543791332, 0.05644026923350467, 0.056433366090770515, 0.05642647597320331, 0.05641959884458242, 0.05641273466897008, 0.05640588341070717, 0.05639904503440896, 0.05639221950496121, 0.05638540678751615, 0.05637860684748858, 0.056371819650551894, 0.05636504516263454, 0.05635828334991603, 0.05635153417882347, 0.05634479761602789, 0.05633807362844066, 0.05633136218321008, 0.056324663247717996, 0.05631797678957624, 0.05631130277662355, 0.05630464117692215, 0.056297991958754665, 0.05629135509062089, 0.056284730541234666, 0.05627811827952098, 0.05627151827461283, 0.0562649304958483, 0.05625835491276777, 0.05625179149511093, 0.05624524021281401, 0.05623870103600713, 0.05623217393501149, 0.05622565888033667, 0.05621915584267811, 0.05621266479291449, 0.056206185702105234, 0.05619971854148787, 0.05619326328247578, 0.05618681989665565, 0.05618038835578517, 0.05617396863179063, 0.05616756069676472, 0.05616116452296419, 0.05615478008280768, 0.05614840734887347, 0.05614204629389748, 0.05613569689077096, 0.0561293591125386, 0.056123032932396344, 0.05611671832368947, 0.05611041525991056, 0.05610412371469758, 0.056097843661832, 0.05609157507523687, 0.05608531792897493, 0.05607907219724691, 0.056072837854389615, 0.05606661487487427, 0.05606040323330473, 0.056054202904415734, 0.05604801386307135, 0.056041836084263226, 0.056035669543109, 0.056029514214850695, 0.05602337007485319, 0.05601723709860266, 0.05601111526170498, 0.05600500453988435, 0.05599890490898177, 0.05599281634495359, 0.055986738823870105, 0.055980672321914095, 0.055974616815379595, 0.055968572280670384, 0.055962538694298715, 0.05595651603288404, 0.05595050427315166, 0.0559445033919315, 0.05593851336615685, 0.05593253417286313, 0.05592656578918666, 0.05592060819236354, 0.05591466135972839, 0.05590872526871329, 0.05590279989684662, 0.05589688522175184, 0.055890981221146614, 0.05588508787284151, 0.055879205154739084, 0.05587333304483278, 0.05586747152120588, 0.055861620562030555, 0.05585578014556684, 0.05584995025016163, 0.05584413085424776, 0.05583832193634303, 0.0558325234750493, 0.0558267354490515, 0.05582095783711688, 0.05581519061809395, 0.05580943377091164, 0.055803687274578614, 0.05579795110818212, 0.05579222525088745, 0.055786509681936956, 0.05578080438064925, 0.05577510932641848, 0.05576942449871346, 0.055763749877076975, 0.05575808544112503, 0.05575243117054599, 0.05574678704509999, 0.05574115304461813, 0.055735529149001775, 0.05572991533822185, 0.0557243115923182, 0.05571871789139883, 0.05571313421563932, 0.05570756054528211, 0.055701996860635865, 0.055696443142074864, 0.055690899370038335, 0.05568536552502988, 0.05567984158761686, 0.055674327538429685, 0.05566882335816142, 0.055663329027567085, 0.055657844527463085, 0.05565236983872668, 0.05564690494229538, 0.0556414498191665, 0.05563600445039652, 0.055630568817100635, 0.05562514290045211, 0.05561972668168197, 0.05561432014207832, 0.05560892326298588, 0.055603536025805575, 0.055598158411993996, 0.05559279040306291, 0.055587431980578784, 0.055582083126162425, 0.05557674382148841, 0.055571414048284674, 0.05556609378833211, 0.055560783023464094, 0.05555548173556606, 0.055550189906575106, 0.05554490751847952, 0.055539634553318486, 0.05553437099318153, 0.055529116820208266, 0.05552387201658793, 0.055518636564558965, 0.05551341044640875, 0.05550819364447313, 0.05550298614113609, 0.05549778791882936, 0.05549259896003212, 0.05548741924727061, 0.05548224876311773, 0.055477087490192804, 0.0554719354111612, 0.055466792508733924, 0.05546165876566746, 0.055456534164763226, 0.05545141868886745, 0.05544631232087083, 0.05544121504370806, 0.055436126840357744, 0.055431047693841926, 0.05542597758722593, 0.055420916503617974, 0.05541586442616892, 0.055410821338071965, 0.05540578722256242, 0.05540076206291734, 0.055395745842455345, 0.05539073854453634, 0.05538574015256118, 0.05538075064997147, 0.055375770020249314, 0.05537079824691705, 0.05536583531353694, 0.05536088120371103, 0.05535593590108084, 0.05535099938932713, 0.055346071652169704, 0.0553411526733671, 0.05533624243671647, 0.05533134092605322, 0.055326448125250914, 0.05532156401822096, 0.05531668858891247, 0.055311821821311946, 0.055306963699443185, 0.05530211420736701, 0.055297273329180996, 0.05529244104901939, 0.0552876173510529, 0.05528280221948834, 0.05527799563856866, 0.055273197592572584, 0.05526840806581448, 0.055263627042644155, 0.055258854507446706, 0.05525409044464235, 0.05524933483868614, 0.05524458767406786, 0.05523984893531189, 0.055235118606976955, 0.05523039667365596, 0.05522568311997589, 0.055220977930597534, 0.055216281090215466, 0.05521159258355773, 0.055206912395385714, 0.055202240510494154, 0.05519757691371069, 0.055192921589895944, 0.055188274523943294, 0.05518363570077869, 0.05517900510536053, 0.05517438272267951, 0.055169768537758505, 0.055165162535652366, 0.055160564701447826, 0.05515597502026334, 0.055151393477248956, 0.05514682005758613, 0.055142254746487665, 0.05513769752919755, 0.0551331483909908, 0.055128607317173346, 0.055124074293081866, 0.055119549304083755, 0.05511503233557687, 0.05511052337298953, 0.05510602240178027, 0.05510152940743782, 0.05509704437548093, 0.05509256729145828, 0.05508809814094827, 0.05508363690955906, 0.05507918358292834, 0.05507473814672324, 0.055070300586640204, 0.05506587088840496, 0.05506144903777225, 0.05505703502052585, 0.055052628822478474, 0.05504823042947156, 0.05504383982737522, 0.05503945700208816, 0.05503508193953754, 0.055030714625678864, 0.05502635504649593, 0.05502200318800065, 0.055017659036233034, 0.055013322577261034, 0.055008993797180404, 0.05500467268211479, 0.05500035921821539, 0.054996053391661005, 0.05499175518865794, 0.05498746459543984, 0.054983181598267664, 0.0549789061834296, 0.054974638337240846, 0.05497037804604376, 0.0549661252962075, 0.054961880074128146, 0.0549576423662285, 0.054953412158958034, 0.054949189438792796, 0.05494497419223534, 0.05494076640581463, 0.05493656606608597, 0.05493237315963089, 0.05492818767305708, 0.05492400959299837, 0.05491983890611449, 0.05491567559909121, 0.05491151965864005, 0.05490737107149833, 0.05490322982442909, 0.054899095904220915, 0.05489496929768798, 0.05489084999166989, 0.05488673797303166, 0.054882633228663574, 0.05487853574548118, 0.054874445510425175, 0.05487036251046136, 0.054866286732580524, 0.05486221816379847, 0.05485815679115577, 0.0548541026017179, 0.054850055582575, 0.05484601572084191, 0.0548419830036581, 0.05483795741818747, 0.05483393895161846, 0.0548299275911639, 0.054825923324060916, 0.05482192613757092, 0.05481793601897949, 0.05481395295559637, 0.05480997693475535, 0.05480600794381422, 0.0548020459701547, 0.054798091001182415, 0.05479414302432678, 0.054790202027040935, 0.05478626799680179, 0.05478234092110976, 0.05477842078748891, 0.0547745075834868, 0.054770601296674395, 0.05476670191464608, 0.05476280942501958, 0.054758923815435796, 0.05475504507355896, 0.05475117318707636, 0.0547473081436984, 0.05474344993115854, 0.054739598537213205, 0.054735753949641676, 0.05473191615624621, 0.05472808514485178, 0.054724260903306156, 0.05472044341947975, 0.05471663268126573, 0.05471282867657969, 0.0547090313933599, 0.05470524081956701, 0.05470145694318413, 0.05469767975221677, 0.054693909234692674, 0.05469014537866194, 0.05468638817219683, 0.05468263760339178, 0.05467889366036329, 0.05467515633125, 0.05467142560421251, 0.05466770146743334, 0.054663983909116975, 0.054660272917489705, 0.05465656848079964, 0.05465287058731666, 0.05464917922533233, 0.05464549438315985, 0.054641816049134054, 0.05463814421161133, 0.05463447885896955, 0.05463081997960807, 0.05462716756194763, 0.05462352159443039, 0.054619882065519716, 0.054616248963700355, 0.05461262227747823, 0.05460900199538041, 0.054605388105955124, 0.054601780597771675, 0.05459817945942039, 0.05459458467951261, 0.05459099624668059, 0.05458741414957748, 0.0545838383768773, 0.054580268917274875, 0.05457670575948582, 0.05457314889224635, 0.054569598304313474, 0.0545660539844648, 0.054562515921498494, 0.05455898410423328, 0.054555458521508345, 0.05455193916218337, 0.05454842601513845, 0.05454491906927401, 0.05454141831351079, 0.054537923736789846, 0.054534435328072464, 0.0545309530763401, 0.054527476970594374, 0.054524006999857044, 0.054520543153169884, 0.05451708541959473, 0.054513633788213396, 0.054510188248127645, 0.05450674878845912, 0.05450331539834934, 0.054499888066959656, 0.05449646678347117, 0.054493051537084725, 0.05448964231702087, 0.05448623911251984, 0.05448284191284145, 0.054479450707265106, 0.05447606548508972, 0.054472686235633755, 0.054469312948235114, 0.0544659456122511, 0.05446258421705838, 0.05445922875205301, 0.05445587920665035, 0.05445253557028493, 0.05444919783241064, 0.05444586598250044, 0.054442540010046496, 0.05443921990456002, 0.0544359056555714, 0.054432597252629965, 0.05442929468530409, 0.054425997943181044, 0.05442270701586708, 0.05441942189298729, 0.05441614256418564, 0.05441286901912488, 0.05440960124748651, 0.054406339238970806, 0.05440308298329671, 0.054399832470201845, 0.054396587689442416, 0.054393348630793245, 0.05439011528404767, 0.05438688763901759, 0.054383665685533336, 0.0543804494134437, 0.054377238812615865, 0.05437403387293539, 0.054370834584306166, 0.05436764093665037, 0.054364452919908414, 0.05436127052403898, 0.05435809373901896, 0.05435492255484332]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.1.4 Step -4- Evaluate the Model:\n",
        "Implementation in the Code:\n",
        "Complete the following code or write your own:\n",
        "\n",
        "Code for RMSE:\n",
        "# Model Evaluation - RMSE\n",
        "def rmse(Y, Y_pred):\n",
        "\"\"\"\n",
        "This Function calculates the Root Mean Squres.\n",
        "Input Arguments:\n",
        "Y: Array of actual(Target) Dependent Varaibles.\n",
        "Y_pred: Array of predeicted Dependent Varaibles.\n",
        "Output Arguments:\n",
        "rmse: Root Mean Square.\n",
        "\"\"\"\n",
        "rmse = # Your Code Here\n",
        "return rmse"
      ],
      "metadata": {
        "id": "-XXUmon0ftU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Model Evaluation - RMSE\n",
        "def rmse(Y, Y_pred):\n",
        "    \"\"\"\n",
        "    This Function calculates the Root Mean Square Error (RMSE).\n",
        "\n",
        "    Input Arguments:\n",
        "    Y: Array of actual (target) dependent variables.\n",
        "    Y_pred: Array of predicted dependent variables.\n",
        "\n",
        "    Output Arguments:\n",
        "    rmse: Root Mean Square Error.\n",
        "    \"\"\"\n",
        "    # Calculate RMSE\n",
        "    rmse_value = np.sqrt(np.mean((Y_pred - Y) ** 2))  # Calculate RMSE\n",
        "    return rmse_value\n"
      ],
      "metadata": {
        "id": "ZUiD6lbsgF-8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions using the final parameters (W_final) obtained from training with the correct number of features\n",
        "\n",
        "W_optimal = W_final  # Assign the final weights from gradient descent to W_optimal\n",
        "Y_pred = np.dot(X_test, W_optimal)  # Use W_optimal which has the correct shape for 2 features\n",
        "\n",
        "# Flatten Y_pred to a 1D array\n",
        "Y_pred = Y_pred.flatten()\n",
        "\n",
        "# Calculate RMSE\n",
        "error = rmse(Y_test, Y_pred)\n",
        "\n",
        "print(\"Root Mean Square Error (RMSE):\", error)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-V0e5LpgHw8",
        "outputId": "108cf0e1-fbe5-4cf2-f8d4-8080a7ec1080"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root Mean Square Error (RMSE): nan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementation in the Code:\n",
        "Complete the following code or write your own:\n",
        "\n",
        "Code for R-Squared Error:\n",
        "\n",
        "**Model Evaluation - R2**\n",
        "\n",
        "def r2(Y, Y_pred):\n",
        "\"\"\"\n",
        "This Function calculates the R Squared Error.\n",
        "\n",
        "Input Arguments:\n",
        "Y: Array of actual(Target) Dependent Varaibles.\n",
        "Y_pred: Array of predeicted Dependent Varaibles.\n",
        "Output Arguments:\n",
        "rsquared: R Squared Error.\n",
        "\"\"\"\n",
        "mean_y = np.mean(Y)\n",
        "ss_tot = # Your Code Here\n",
        "ss_res = # Your Code Here\n",
        "r2 = # Your Code Here\n",
        "return r2"
      ],
      "metadata": {
        "id": "tQVK0XKfgfkQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Evaluation - R²\n",
        "def r2(Y, Y_pred):\n",
        "    \"\"\"\n",
        "    This Function calculates the R Squared Error.\n",
        "\n",
        "    Input Arguments:\n",
        "    Y: Array of actual (target) dependent variables.\n",
        "    Y_pred: Array of predicted dependent variables.\n",
        "\n",
        "    Output Arguments:\n",
        "    r2: R Squared Error.\n",
        "    \"\"\"\n",
        "    mean_y = np.mean(Y)  # Calculate the mean of actual values\n",
        "    ss_tot = np.sum((Y - mean_y) ** 2)  # Total sum of squares\n",
        "    ss_res = np.sum((Y - Y_pred) ** 2)  # Residual sum of squares\n",
        "    r2 = 1 - (ss_res / ss_tot)  # R-squared calculation\n",
        "    return r2\n"
      ],
      "metadata": {
        "id": "LTa5iKw9gi8F"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your dataset\n",
        "X = data[['Math', 'Reading']].values  # Shape (n_samples, n_features)\n",
        "Y = data['Writing'].values.reshape(-1, 1)  # Shape (n_samples, 1)\n",
        "\n",
        "# Split into training and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize weights for two features\n",
        "W_initial = np.random.rand(2, 1)  # Shape (2, 1) for Math and Reading\n",
        "\n",
        "# Set hyperparameters\n",
        "alpha = 0.01\n",
        "iterations = 1000\n",
        "\n",
        "# Perform gradient descent\n",
        "final_params, cost_history = gradient_descent(X_train, Y_train, W_initial, alpha, iterations)\n",
        "\n",
        "# Make predictions using the final parameters\n",
        "Y_pred = np.dot(X_test, final_params).flatten()  # Flatten to get a 1D array\n",
        "\n",
        "# Calculate R-squared\n",
        "r_squared = r2(Y_test.flatten(), Y_pred)  # Flatten Y_test for comparison\n",
        "\n",
        "print(\"R-squared:\", r_squared)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4DVOz7tg5nw",
        "outputId": "d63ebf12-3f62-443d-a1e8-acdaefe0fa23"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R-squared: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:88: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "<ipython-input-10-61cbcff67d3b>:5: RuntimeWarning: overflow encountered in square\n",
            "  cost = (1 / (2 * len(Y))) * np.sum(error ** 2)  # MSE formula\n",
            "<ipython-input-10-61cbcff67d3b>:41: RuntimeWarning: invalid value encountered in subtract\n",
            "  W_update = W - alpha * dw  # Update weights\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.1.5 Step -5- Main Function to Integrate All Steps:\n"
      ],
      "metadata": {
        "id": "YlaIm_xXiWun"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the cost function\n",
        "def cost_function(X, Y, W):\n",
        "    Y_pred = np.dot(X, W)  # Predicted values\n",
        "    error = Y_pred - Y  # Calculate error\n",
        "    cost = (1 / (2 * len(Y))) * np.sum(error ** 2)  # MSE formula\n",
        "    return cost\n",
        "\n",
        "# Define the gradient descent function\n",
        "def gradient_descent(X, Y, W, alpha, iterations):\n",
        "    cost_history = [0] * iterations\n",
        "    m = len(Y)\n",
        "\n",
        "    for iteration in range(iterations):\n",
        "        Y_pred = np.dot(X, W)  # Predicted values\n",
        "        loss = Y_pred - Y  # Calculate loss\n",
        "        dw = (1 / m) * np.dot(X.T, loss)  # Gradient calculation\n",
        "        W -= alpha * dw  # Update weights\n",
        "        cost_history[iteration] = cost_function(X, Y, W)  # Store cost\n",
        "\n",
        "    return W, cost_history\n",
        "\n",
        "# Model Evaluation - RMSE\n",
        "def rmse(Y, Y_pred):\n",
        "    return np.sqrt(np.mean((Y_pred - Y) ** 2))  # Calculate RMSE\n",
        "\n",
        "# Model Evaluation - R²\n",
        "def r2(Y, Y_pred):\n",
        "    mean_y = np.mean(Y)\n",
        "    ss_tot = np.sum((Y - mean_y) ** 2)\n",
        "    ss_res = np.sum((Y - Y_pred) ** 2)\n",
        "    return 1 - (ss_res / ss_tot)\n",
        "\n",
        "# Main Function\n",
        "def main():\n",
        "    # Split the data into features (X) and target (Y)\n",
        "    X = data[['Math', 'Reading']].values  # Features: Math and Reading marks\n",
        "    Y = data['Writing'].values.reshape(-1, 1)  # Target: Writing marks\n",
        "\n",
        "    # Split the data into training and test sets (80% train, 20% test)\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Initialize weights (W) to zeros, learning rate and number of iterations\n",
        "    W_initial = np.zeros((X_train.shape[1], 1))  # Initialize weights for two features\n",
        "    alpha = 0.00001  # Learning rate\n",
        "    iterations = 1000  # Number of iterations for gradient descent\n",
        "\n",
        "    # Perform Gradient Descent\n",
        "    W_optimal, cost_history = gradient_descent(X_train, Y_train, W_initial, alpha, iterations)\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    Y_pred = np.dot(X_test, W_optimal).flatten()  # Flatten to get a 1D array\n",
        "\n",
        "    # Evaluate the model using RMSE and R-Squared\n",
        "    model_rmse = rmse(Y_test.flatten(), Y_pred)  # Flatten for comparison\n",
        "    model_r2 = r2(Y_test.flatten(), Y_pred)      # Flatten for comparison\n",
        "\n",
        "    # Output the results\n",
        "    print(\"Final Weights:\", W_optimal.flatten())\n",
        "    print(\"Cost History (First 10 iterations):\", cost_history[:10])\n",
        "    print(\"RMSE on Test Set:\", model_rmse)\n",
        "    print(\"R-Squared on Test Set:\", model_r2)\n",
        "\n",
        "# Execute the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXyds1CCipyz",
        "outputId": "875c30b7-9283-4f07-8979-ad1c5b705b2c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Weights: [0.34811659 0.64614558]\n",
            "Cost History (First 10 iterations): [2013.165570783755, 1640.286832599692, 1337.0619994901588, 1090.4794892850578, 889.9583270083234, 726.8940993009545, 594.2897260808594, 486.4552052951635, 398.7634463599484, 327.4517147324688]\n",
            "RMSE on Test Set: 5.2798239764188635\n",
            "R-Squared on Test Set: 0.8886354462786421\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Present your finding:**\n",
        "\n",
        "1. Did your Model Overfitt, Underfitts, or performance is acceptable.\n",
        "2. Experiment with different value of learning rate, making it higher and lower, observe the result."
      ],
      "metadata": {
        "id": "GH6VkR7Oixa4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model's performance is acceptable, as indicated by a reasonable RMSE and R-squared value, suggesting that it neither overfits nor underfits the data.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Experimenting with different learning rates showed that a higher learning rate led to divergence in the cost function, while a lower learning rate resulted in slower convergence but more stable training."
      ],
      "metadata": {
        "id": "Z5Tru7PQiz23"
      }
    }
  ]
}